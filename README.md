# UnstructuredTextAnalysis

## Part 1 - Preprocessing the data
### Steps involved
* Removing punctuation
* Removing numbers
* Converting text to lower case
* Removing stop words
> download the 'stopwords' package from nltk.download()

## Part 2 - WordCloud on the dataset
### Steps involved
* Installing the WordCloud python toolkit | checkout: [https://github.com/amueller/word_cloud]
* Plotting the WordClouds for the 4 data sets used

## Part 3 - Word and sentence tokenization
### Steps involved
* Word tokenization is done with 'word_tokenize' module in the nltk package
* Sentence tokenization is done with 'sent_tokenize' module in the nltk.tokenize package
> download the 'punkt' package from nltk.download()

## Part 4 - Tagging parts of speech
### Steps involved

## Part 5 - Stemming and lemmatization
### Steps involved

## Part 6 - Applying Stanford Named Entity Recognizer
### Steps involved